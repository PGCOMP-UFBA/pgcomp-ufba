\section{Conjuntos e Lógica Fuzzy}
\subsection{Considerações iniciais}
Primeiramente introduzida em \cite{Zadeh1965}, onde o autor inicia a discussão definindo os
conjuntos fuzzy, sendo uma classe de objetos com valores contínuos de pertinência. Cada conjunto é então caracterizado por uma função de pertinência, a qual atribui a cada objeto do conjunto um grau de 
pertinência que varia entre zero e um. As operações matemáticas da teoria dos conjuntos, como
inclusão, união, intersecção, complemento, relação, etc., também são estendidas aos conjuntos fuzzy,
assim como várias propriedades dessas notações são definidas.

Uma das motivações da lógica fuzzy, vem da maneira como nosso cérebro classifica e rotula o mundo
real. Por exemplo, ao rotularmos uma pessoa como alta, estamos atribuindo ela ao grupo de pessoas 
altas. Assim como quando nos expressamos sobre o quanto um determinado dia está fazendo calor ou 
frio. O conjunto de pessoas altas ou dias frios, não se enquadra na sua totalidade na lógica 
clássica. Pois essa forma imprecisa de descrever o mundo a nossa volta, desempenha
um papel fundamental na forma de pensar humana, assim como também nas áreas de reconhecimento 
de padrões, comunicação e abstração\cite{Zadeh1965}.
Portanto esta seção tem como propósito contextualizar os principais aspectos da lógica
fuzzy que a torna tão importante no contexto da organização flexível de documentos. Portanto definições mais aprofundadas sobre fuzzy fogem do escopo desse texto .
% TODO: Necessário informar o leitor sobre isso? Verificar com Tati

\subsection{Definição de conjuntos fuzzy}

Seja X um espaço de objetos, com um elemento genérico {\it x\/}. 
Sendo $X= \big\{x\big\}$.

Um conjunto fuzzy $A$ em $X$ é caracterizado por uma função de pertinência $f_A(x)$, a qual associa
a cada elemento de $X$ um número real presente no intervalo de $[0,1]$, sendo o valor de $f_A(x)$
a representação do grau de pertinência de $x$ em $A$.

\subsection{Lógica fuzzy}

A lógica fuzzy é uma lógica multi valorada, onde os valores das variáveis pertencem ao intervalo
de [0,1], enquanto na lógica clássica os valores verdades só possuem os estados 0 ou 1 (também conhecido como valores {\it crisp\/}). Uma da mais importantes aplicações está no tratamento de precisão e incerteza. O que nos permite a modelar soluções mais adequadas para ambientes imprecisos e incertos.
Antes da lógica fuzzy ser introduzida em \cite{Zadeh1965}, em 1930 Lukasiewics\cite{Chen2000} 
desenvolveu
a lógica n-valorada para $3 < n < \infty$, utilizando apenas os operadores lógicos de negação $-$ e implicação $\Rightarrow$. Dado então um inteiro positivo, $n > 3$, a lógica n-valorada assume 
valores verdade pertencente ao intervalo $[0,1]$, definidos pela seguinte partição igualmente 
espaçada: 
$$0 =  \frac{0}{n-1}, \frac{1}{n-1},\frac{2}{n-1},...,\frac{n-2}{n-1},\frac{n-1}{n-1} = 1$$
Para estender a lógica n-valorada para uma lógica com infinitos valores $2 \leq n \leq \infty$, 
\cite{Zadeh1965} modificou a lógica de Lukasiewics definindo os seguintes operadores lógicos:
$$\bar{a} = 1 -a$$
$$a \wedge b = min\{a,b\}$$
$$a \vee b = max\{a,b\}$$
$$a \Rightarrow b = min\{1, 1+b-a\}$$
$$a \Leftrightarrow  b = 1 - |a-b|$$

O objetivo da lógica fuzzy é prover mecanismos para tratar imprecisão e incerteza, se baseando na
teoria de conjuntos fuzzy e usando proposições imprecisas, de modo similar a lógica clássica 
usando proposições precisas baseadas na teoria dos conjuntos. 
Para entendermos essa noção, vejamos então um mesmo exemplo pela ótica do raciocínio clássico e 
em seguida usando as ferramentas para descrever imprecisão da lógica fuzzy. 
\begin{enumerate}[label=\alph*)]
  \item Todo texto com 100 palavras ou mais da área jurídica, tem como assunto o direito.
  \item O texto A com título as manifestações de junho, tem 100 palavras da área jurídica.
  \item O texto B com título política nas universidades, tem 99 palavras da área jurídica. 
  \item O texto A tem como assunto o direito e o texto B não tem como assunto o direito.
\end{enumerate}
Essa série de proposições ilustra o raciocínio empregado na lógica clássica, e seguindo as regras
de inferência conseguimos verificar que as sentenças estão corretas. No entanto é fácil notar que
a sentença d) não expressa muito bem o nosso entendimento sobre a temática dos textos.
Seria comum alguém substituir a sentença d), por e) O texto B fala um pouco sobre direito. 
Vamos então adicionar a imprecisão comum no mundo real as sentenças anteriores.
\begin{enumerate}[label=\alph*)]
  \item Todo texto que tem entre 50 e 100 palavras da área jurídica fala um pouco sobre direito. 
Enquanto todo texto que contenha 100 ou mais palavras da área jurídica fala bastante sobre direito.
  \item O texto A com título as manifestações de junho, tem 100 palavras da área jurídica.
  \item O texto B com título política nas universidades, tem 99 palavras da área jurídica. 
  \item O texto A fala bastante sobre direito, enquanto o texto B fala um pouco sobre direito.
\end{enumerate}
Esse tipo de dedução comumente utilizada no nosso dia a dia, não tem como ser tratada pela lógica
clássica. No entanto podemos lidar com esse tipo de inferência imprecisa, empregando a lógica fuzzy, a qual permite o uso de alguns termos linguísticos imprecisos como:
\begin{itemize}
  \item Predicados fuzzy: antigo, raro, caro, alto, rápido
  \item Quantificadores fuzzy: muito, pouco, quase, alguns
  \item Graus de verdade fuzzy: totalmente verdadeiro, verdadeiro, parcialmente falso, falso, definitivamente falso
\end{itemize}

%\section{Organização Flexível de Documentos}
%
%A organização de documentos tem sido uma área de fundamental importância no campo da
%mineração de dados. Pois na era da informação, onde temos mídias sociais, internet das coisas
%e computação móvel, produzindo diariamente uma elevada quantidade de dados estruturados e 
%não estruturados. 
%
%Se nos concentrarmos nos dados não estruturados, temos os dados textuais,
%que por sua vez pertencem a uma ampla gama de tópicos que é constantemente atualizada, de maneira
%que a organização deles em categorias não pode ser predefinida \cite{Carvalho2016}. 
%Diante deste contexto, esse capítulo visa fundamentar as bases da organização flexível de documentos 
%seguindo o modelo defendido em \cite{Nogueira2013},


\section{Pré-Processamento}
Pré-processamento dos dados é o processo de limpeza e preparação do texto para classificação.
Assim como muitas palavras em um texto não causam nenhum impacto no significado geral do documento\cite{Haddi2013}.
Soma se a isso o enorme custo computacional do processo de mineração de textos, devido a grande quantidade de verbetes presente em dados textuais. 
Portanto quanto maior for a coleção de textos, maior será a quantidade de palavras distintas. 
Elevando bastante o custo computacional das tarefas de agrupamento e classificação, 
que por sua vez são baseadas na análise do vocabulário dos documentos. 
Com isso, vários pesquisadores propuseram métodos para tentar simplificar, sintetizar e eliminar redundâncias desnecessárias nas coleções de textos.
Pois, quanto mais compacto for a quantidade de verbetes da coleção de documentos, menor o custo
computacional e a quantidade de memória utilizada nas fases de agrupamento, extração de descritores
e classificação. A esse conjunto de técnicas realizadas inicialmente sobre os documentos, 
denominamos de pré-processamento. 

A fase de pré-processamento voltada para a mineração de textos, requer técnicas muito diferentes
no preparo dos dados não estruturados para as fases posteriores, do que as técnicas comumente
encontradas nos métodos de descoberta de informação. As quais visam preparar dados estruturados para
as clássicas operações de mineração de dados \cite{Feldman2007}.

Segundo \cite{Feldman2007}, é possível categorizar de maneira clara as técnicas de pré-processamento de textos em duas categorias, de acordo com as tarefas realizadas pela técnica e através dos 
algoritmos e frameworks que a mesma utiliza. Por sua vez, as técnicas categorizadas pelas suas 
tarefas, geralmente visam realizar a estruturação do documento através de tarefas e sub tarefas. 
Como por exemplo, realizar a extração de título e sub título de documentos no formato PDF.
No entanto, as demais técnicas de pré-processamento são derivadas de métodos formais, e incluem 
esquemas de classificação, modelos probabilísticos e sistemas baseado em regras.

O processo de pré-processamento de dados textuais, inicia com um documento parcialmente estruturado 
e avança incrementando a estrutura através do refinamento das características do documento e
adicionando novas \cite{Feldman2007}. 
No contexto da mineração de textos as características dos documentos são as 
suas palavras\cite{Haddi2013}. Ao final do processo, as palavras mais relevantes são utilizadas, e as demais
são descartadas. Uma vez que manter estas palavras torna a dimensionalidade do problema maior, pois 
cada palavra no texto é tratada como uma dimensão\cite{Haddi2013}.

O processo como um todo envolve várias etapas, as quais podemos elencar a remoção de espaços, 
expansão de abreviações, remoção de $stopwords$, que são palavras que não possuem relevância no 
significado geral do texto e geralmente são compostas por proposições, pronomes, artigos,
interjeições dentre outras\cite{Nogueira2013}. Assim como também o processo de $stemming$ ou 
lematização, onde se busca encontrar o radical da palavra, visando assim remover palavras que 
possuam significados similares. Ainda é possível usar as técnicas de NLP 
($Natural Language Processing$) para eliminar sinônimos. Por fim é realizada a seleção de 
termos \cite{Haddi2013}. 

Diversos métodos foram então propostos para se capturar a importância dos termos em coleções 
textuais. Sendo o método { \it Term Frequency Inverse 
Document Frequency\/ }(TF-IDF) um dos mais importantes\cite{Haddi2013} e frequentemente utilizado na literatura. A definição da TF-IDF está na equação (\ref{eq:tfidf}), onde $N$ é o número de 
documentos da coleção, $DF$ o total de documentos que possuem este termo e $FF$ ({\it frequency feature}) a frequência do termo no documento.

\begin{equation}
  \varphi(t,d) = FF * log(\frac{N}{DF}) 
 \label{eq:tfidf}
\end{equation}

Como resultado final de todo o processo de pré-processamento, obtém-se a matrix $D$. Onde $D$ 
representa os $n$ documentos da coleção, sendo cada documento $d_{i}$, com $1 \leq n \leq N$, 
uma linha da matriz $D$, definido como sendo 
$ d_{i} = [\varphi(t_{1},d_{i}),\varphi(t_{2},d_{i}),\varphi(t_{3},d_{i}),...,\varphi(t_{k},d_{i})] $,
onde $t_{j}$ é um termo presente na coleção, com $1 \leq j \leq k$.

\section{Agrupamento Fuzzy}
O agrupamento é um processo não supervisionado \cite{Feldman2007}, onde o objetivo é organizar os 
documentos similares no mesmo grupo e os documentos com grau de dissimilaridade elevado em 
grupos distintos\cite{Nogueira2013}\cite{Feldman2007}. Este processo é de grande 
utilidade para diversos campos de estudo da inteligência computacional, 
como a mineração de dados, recuperação de informação, 
segmentação de imagens e classificação de padrões \cite{Feldman2007}.

O problema de organizar os documentos de maneira a maximizar a similaridade entre os membros
de um mesmo grupo, e minimizar a similaridade entre documentos de grupos distintos, é 
essencialmente um problema de otimização \cite{Feldman2007}. 
Então pretende-se otimizar a escolha dos grupos, entre todos as possibilidades de agrupamento, 
dada uma função objetivo que captura a qualidade dos grupos. Esta função
é responsável por atribuir ao conjunto de possíveis grupos um número real, de maneira que quanto 
melhor for os grupos, maior será o seu valor\cite{Feldman2007}.

A medida de similaridade desempenha um papel fundamental no agrupamento, uma vez que ela 
precisa expressar o quanto distante está um elemento do outro na coleção. Assim sendo, para 
obtermos bons resultados durante a organização dos elementos é de grande importância a escolha
adequada da medida de similaridade, e esta escolha precisa ser feita de acordo com o tipo dos dados.
Na literatura a medida de similaridade mais popular\cite{Feldman2007} é a distância 
euclidiana (Equação \ref{eq:euclidiana}),
que tem se mostrado bastante adequada em dados com baixa dimensionalidade.

\begin{equation}
D(x_{i}, x_{j}) = \sqrt{\sum_k{(x_{ik}-x_{jk})^2}}
\label{eq:euclidiana}
\end{equation}

No entanto, em coleções textuais a matriz documentos x termos é naturalmente esparsa, 
devido a grande variedade de 
verbetes em uma coleção, o que faz com que um determinado documento $d_{i}$, não contenha diversos
termos presentes em um outro documento $d_{j}$. Resultando assim que o vetor de características de 
cada documento, seja preenchido com vários zeros. Reduzindo então a eficácia da distância 
euclidiana (Equação \ref{eq:euclidiana})
\cite{Nogueira2013}. Consequentemente a medida de similaridade mais comum para coleções textuais
é o coeficiente de similaridade de cosseno \cite{Nogueira2013}\cite{Feldman2007}. 
Por sua vez o coeficiente
de similaridade de cosseno, desconsidera os diversos zeros presentes nos vetores de termos 
dos documentos, levando em conta apenas o ângulo formado entre eles\cite{Nogueira2013}.
Na equação (\ref{eq:simcos}) temos a definição do coeficiente de similaridade de cosseno, onde 
$d_1$ e $d_2$, são dois documentos quaisquer da coleção de documentos, e $1 \leq t \leq k$, onde 
$k$ é a quantidade total de termos da coleção, e $d_{ik}$ a frequência do termo $t$ no documento $d_i$.

\begin{equation}
  scos(d_{1}, d_{2}) = cos\theta = \frac{d_{1} \cdot d_{2}}{|d_{1}||d_{2}|} = \sum_{t=1}^k{\varphi(d_{1t},d_1) \cdot \varphi(d_{2t},d_2)} \in [0,1]
  \label{eq:simcos}
\end{equation}

Os grupos resultantes desse processo, podem possuir algumas características que estão diretamente
relacionadas com o método de agrupamento empregado. Estes podem ser $hard$ ou $crisp$, caso 
o método de agrupamento seja baseado na lógica clássica, assim como podem ser $soft$, 
caso o método seja baseado na lógica fuzzy. No agrupamento $hard$, cada documento 
$d_{i}$ só poderá pertencer a um único grupo $g_{j}$ \cite{Bezdek1984}. Enquanto em grupos $soft$,
cada documento $d_{i}$ pode pertencer a um ou mais grupos $g_{j}$, com grau de pertinência variados.
Além destes, os grupos ainda podem ser $flat$ ou hierárquicos, onde no agrupamento $flat$
todos os grupos estão no mesmo nível, enquanto no modelo hierárquico os grupos podem estar dispostos
em uma hierarquia, de modo que uma relação de parentesco é definida entre eles.

Portanto, seja $G = \{g_{1},g_{2},g_{3},...,g_{c}\}$ os grupos resultantes do agrupamento, 
sendo $c$ o total de grupos. No agrupamento $hard$, a pertinência de cada 
documento $d_{i}$ pode ser representada pela função de pertinência 
$\kappa(d_{i}, g_{j}) \in \{0,1\}$, tal que $\sum_{j=1}^c \kappa(d_{i}, g_{j}) = 1$. 
Um dos mais populares algoritmos a implementar essa abordagem $hard$ é o K Means.
Em \cite{Bezdek1984}\cite{Nogueira2013}\cite{Feldman2007}, é apontado uma falha inerente dessa
abordagem, pois quando um documento só pode pertencer a um único grupo, fica evidenciado que o mesmo
não compartilha nenhuma similaridade com os documentos dos demais grupos, o que não expressa a
imprecisão intrínseca da sobreposição dos assuntos em documentos de texto.

Com o objetivo de tratar essa falha da abordagem $hard$ e adicionar o tratamento de imprecisão 
e incerteza no agrupamento,
\cite{Bezdek1984} utilizou o modelo de partições fuzzy definido em \cite{Zadeh1965}, para permitir
pertinências parciais de um elemento a um grupo, propondo assim o algoritmo Fuzzy C Means (FCM). 
Sendo assim, a função de pertinência de um documento $d_{i}$ em um grupo $g_{j}$, pode ser definida como sendo $\mu(d_{i}, g_{j}) \in [0,1]$, 
tal que $\sum_{j=1}^m \mu(d_{i}, g_{j}) = 1$.

Outro desafio sempre presente em métodos de agrupamento é a descoberta do número ideal de grupos
em uma coleção. O método de organização flexível proposto em \cite{Nogueira2013} 
utiliza a  $Fuzzy\ Silhouette$ (FS) para realizar a validação do agrupamento fuzzy, e por conseguinte
encontrar o número de grupos ideal. A função FS é uma adaptação do método de critério de largura 
média ($Average\ Silhuette\ Width\ Criterion$ - ASWC), desenvolvido para o agrupamento $crisp$
\cite{Nogueira2013}. A definição da silhueta fuzzy 
está nas equações (\ref{eq:silhuette}) e (\ref{eq:fs}), onde $\alpha(d_i, g_l)$ é a distância 
média entre o documento $d_i$ e todos os documentos 
presentes no grupo $g_l$, enquanto 
$\beta(d_i,g_l) = min\{\alpha(d_i,g_h) | 1 \leq h \leq c; h \neq l\}$, é a 
medida de dissimilaridade de $d_i$ ao grupo vizinho mais próximo de $g_l$, tal que $c$ é a 
quantidade de grupos.
\begin{equation}
  S(d_i) = \frac{\beta(d_i, g_l) - \alpha(d_i,g_l)}{max\{\alpha(d_i,g_l), \beta(d_i,g_l)\}}
  \label{eq:silhuette}
\end{equation}
\begin{equation}
  FS = \frac{\sum_{i=1}^n{(\mu_1(d_i) - \mu_2(d_i))}S(d_i)}{\sum_{i=1}^n{(\mu_1(d_i) - \mu_2(d_i))}}
  \label{eq:fs}
\end{equation}
Na equação (\ref{eq:fs}), $\mu_1(d_i)$ é maior pertinência do documento $d_i$ em um grupo, enquanto 
$\mu_2(d_i)$ é a segunda maior. Quanto maior então for o valor da função FS, melhor será o 
agrupamento. Deste modo para encontrar o número de grupos ideal, basta executar a função FS 
variando o número de grupos, e selecionar o agrupamento que tiver o valor máximo de FS.

Toda investigação realizada neste trabalho tomou como base os métodos de agrupamento que 
derivam do algoritmo FCM\cite{Bezdek1984}, para se beneficiar da capacidade de tratar imprecisão e 
incerteza da lógica fuzzy, e por conseguinte permitir que um mesmo documento seja categorizado 
em mais de um tópico, refletindo a realidade dos documentos textuais. Utilizando como medida
de similaridade o coeficiente de similaridade de cosseno (Equação \ref{eq:simcos}). E por fim a quantidade
de grupos ideal foi escolhida utilizando o método da silhueta fuzzy ( Equação \ref{eq:fs}).

\subsection{Algoritmo Fuzzy C Means (FCM)}
\cite{Bezdek1984} descreve um método de agrupamento fuzzy que produz como saída partições 
fuzzy e protótipos dos grupos. Esse algoritmo desempenha um papel importante no
contexto do agrupamento fuzzy, devido seu pioneirismo no campo de estudo, possuindo diversas
extensões. Assim como também é considerado um dos mais amplamente utilizados métodos de agrupamento
fuzzy da literatura\cite{Pal2005}.

Seja então $V = \{v_1,v_2,v_3,...,v_c\}$ os protótipos dos grupos $G = \{g_1,g_2,g_3,...g_c\}$ 
definidos por
\begin{equation}
  V = \left\{ 
    v_j | v_j = \frac{\sum_{i=1}^n[\mu(d_i,g_j)]^m d_i}{\sum_{i=1}^n[\mu(d_i,g_j)]^m}, 
  1 < j \leq c
\right\}
  \label{eq:prototipos}
\end{equation}
, 
tal que $v_i$ seja o protótipo de $g_i$, $c$ o número de grupos gerados no agrupamento e $n$ o
número de documentos presentes na coleção. Bem como
seja 
\begin{equation}
  U_{c \times n} = \{\mu(d_i, g_k) | \mu(d_i, g_k) \in [0,1], 1 < i \leq n, 1 < k \leq c, 
  eqs (\ref{eq:pert_restricao_1})(\ref{eq:pert_restricao_2})\}
  \label{eq:part_fuzzy}
\end{equation}
uma partição fuzzy, com todas as pertinências dos documentos aos grupos. Sendo $m$ 
o fator de fuzificação, que regula o quão fuzzy será as partições
finais. De modo que para $m = 1$ a partição resultante é totalmente $crisp$ 
(figura \ref{fig:cluster_crisp}) e para 
$m \rightarrow \infty$ a interseção entre os grupos tende a 
aumentar (figura \ref{fig:cluster_fuzzy})\cite{Pal2005}\cite{Nogueira2013}. Segundo \cite{Bezdek1984} nenhuma teoria ou evidência
computacional aponta para um valor ótimo de $m$, contudo o autor aponta que a faixa de valores 
ideais aparenta ser [1,30]. Sendo assim, se existir um conjunto de dados para teste, uma boa 
estratégia para a escolha de $m$ é a realização de testes experimentais. Caso contrário,
o intervalo de [1.5, 3.0] aparenta trazer bons resultados para a maior parte dos dados.

\begin{figure}[!htp]
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=0.6\columnwidth]{assets/clusters_crisp.pdf}
    \caption{Ilustração denotando os grupos $g_1,g_2,g_3$ organizados 
      sem sobreposição, para $m = 1$.}
    \label{fig:cluster_crisp}
  \end{minipage}\hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=0.6\columnwidth]{assets/clusters_fuzzy.pdf}
    \caption{Ilustração denotando os grupos $g_1,g_2,g_3$ organizados de maneira fuzzy, 
      com sobreposição, quando $m \rightarrow \infty$.}
    \label{fig:cluster_fuzzy}
  \end{minipage}
\end{figure}
E ademais a função de pertinência $\mu(d_i,g_k)$ de cada documento na coleção, é definida como sendo 
\begin{equation}
  \mu(d_i,g_k) = \frac{1}{\sum_{j=1}^n(\frac{dist(d_i,v_k)}{dist(d_i,v_j)})^{\frac{1}{m-1}}}
  \label{eq:pertinencia}
\end{equation}
, tal que $\mu(d_i, g_k)$ está sujeita as equações
\begin{equation}
  \sum_{k=1}^c \mu(d_i,g_k) = 1
  \label{eq:pert_restricao_1}
\end{equation}
e
\begin{equation}
  0 < \sum_{i=1}^n \mu(d_i,g_k) < n 
  \label{eq:pert_restricao_2}
\end{equation}
. Onde usualmente no contexto do agrupamento de coleções textuais $dist(d_i,g_k) = scos(d_i,g_k)$.
Sendo assim a restrição (\ref{eq:pert_restricao_2}) da equação 
(\ref{eq:pertinencia}) tem como função evitar que algoritmo FCM produza grupos 
vazios\cite{Nogueira2013}. Enquanto a
equação (\ref{eq:pert_restricao_1}) impõe que a soma das pertinências seja sempre igual a um. 
Entretanto a restrição (\ref{eq:pert_restricao_2}) produz um problema em elementos equidistantes aos grupos. Ou seja, quando temos o caso 
$\mu(d_i, g_1) = \mu(d_i, g_2) = ... = \mu(d_i, g_c)$. Nessa situação o grau de pertinência do 
elemento a cada grupo será a pertinência média, ou seja 
$\mu(d_i, g_1) = \mu(d_i, g_2) = ... = \mu(d_i, g_c) = \frac{1}{c}$. Supondo agora um segundo
documento $d_2$, que seja mais distante do que $d_1$, porém assim como $d_2$, também 
equidistante aos grupos, temos que $\mu(d_2, g_j) = \mu(d_1, g_j) = \frac{1}{c}$, para 
$1 < j \leq c$. Nesse
contexto a pertinência de $d_2$ e $d_1$, não expressa a distância relativa desses documentos aos 
grupos. Esse problema está ilustrado na Figura (\ref{fig:fcm_problem}).

\begin{figure}[!htp]
  \centering
  \includegraphics[width=0.4\columnwidth]{assets/clusters_fcm_problem.pdf}
  \caption{Problema dos elementos equidistantes do algoritmo FCM. Na imagem $g_1$ e $g_2$ são grupos,
    com os seus respectivos protótipos $v_1$ e $v_2$. Enquanto $d_1$ e $d_2$ são documentos 
  equidistantes aos protótipos $v_1$ e $v_2$. Portanto 
$\mu(d_1,g_1) = \mu(d_1,g_2) = \mu(d_2,g_1) = \mu(d_2,g_2) = 0.5$.}
  \label{fig:fcm_problem}
\end{figure}

Segundo \cite{Bezdek1984} várias funções de otimização das partições fuzzy produzidas no agrupamento
foram propostas. Sendo a minimização da função objetivo $J(U_{c \times n},G,V,D)$, 
que visa minimizar a distância entre os documentos e os protótipos dos grupos\cite{Nogueira2013}, 
definida na equação (\ref{eq:fcm_obj}) a mais popular.
\begin{equation}
  min\{(J(U_{c \times n},G,V,D) = \sum_{i=1}^n \sum_{j=1}^c [\mu(d_i, g_j)]^m dist(d_i, v_j)\}
  \label{eq:fcm_obj}
\end{equation}
O mais utilizado algoritmo para soluções aproximadas da minimização (\ref{eq:fcm_obj}), é a iteração
de Picard\footnote{Método iterativo para construção de soluções aproximadas, atribuído ao 
matemático francês Charles Emile Picard (1856-1941). 
\url{http://mathfaculty.fullerton.edu/mathews/n2003/PicardIterationMod.html}}\cite{Pal2005} 
através das equações (\ref{eq:prototipos})(\ref{eq:part_fuzzy}). Sendo assim  
o ciclo de aproximações se dá por $V_{t-1} \Rightarrow  U_t \Rightarrow  V_t$, onde ao final
de cada iteração é verificado se $\parallel V_{t-1} - V_t\parallel < \varepsilon$. 
A literatura também expressa que os ciclos podem começar pela partição fuzzy, fazendo então 
$U_{t-1} \Rightarrow  V_t \Rightarrow  U_t$, e ao final do ciclo checando o erro mínimo com 
$\parallel U_{t-1} - U_t\parallel < \varepsilon$, sendo $t$ o contador de iterações. 
Contudo existem benefícios em termos de processamento e memória ao utilizar a iteração iniciando
e finalizando com $V$\cite{Pal2005}.
\cite{Bezdek1984} e \cite{Pal2005} afirmam que convergência desse modelo iterativo ocorre
em ambos os tipos de ciclo.
A partição fuzzy inicial $U_0$ é comumente inicializada
com valores aleatórios\cite{Nogueira2013} ou com o resultado de um agrupamento previamente 
executado\cite{Pal2005}\cite{Krishnapuram1993}. Nas demais iterações a atualização dos protótipos é
realizada a partir da equação (\ref{eq:prototipos}). O pseudo código utilizando essa abordagem
iterativa está listado no algoritmo (\ref{alg:fcm}), no qual 
$\textbf{{\color{blue}inicializa-particao-fuzzy}(D,G)}$, pode ser uma das duas formas
de inicialização descritas anteriormente.

\begin{algorithm}[H]
\SetAlgoLined
\KwData{D, c, m, $\varepsilon$}
\KwResult{V, U}
G $\gets$ $[g_1,g_2,...,g_c]$\;
$U_0$ $\gets$ \textbf{{\color{blue}inicializa-particao-fuzzy}(D,G)}\;
$t \gets 0$\;
\Do{ $\parallel U_{t-1} - U_t\parallel > \varepsilon$ }{
  $V_t$ $\gets$ calcula usando (\ref{eq:prototipos})\;
  $t \gets t + 1$\;
  $U_t$ $\gets$ calcula usando (\ref{eq:part_fuzzy})\;
}
$\textbf{retorne} (U_t, V_t)$\;
\caption{Pseudo código da implementação iterativa do método FCM}
\label{alg:fcm}
\end{algorithm}

\subsection{Algoritmo Possibilistic C Means (PCM)}
\subsection{Algoritmo Possibilistic Fuzzy C Means (PFCM)}
\subsection{Algoritmo Hierarchic Fuzzy C Means (HFCM)}

\section{Extração de descritores}

A tarefa de rotular grupos é um dos problemas chaves do agrupamento de textos, 
pois ao final do processo de agrupamento, os grupos precisam apresentar alguma relevância para 
o usuário\cite{Zhang2008}. Assim como pretend-se que os descritores escolhidos também sejam 
significativos para os documentos presentes no grupo a ser rotulado. 

Essa etapa pode ser realizada manualmente, com o usuário guiando o processo, ou de forma 
automatizada, que por sua vez é mais interessante para a proposta de organização flexível de 
documentos. Uma vez que para grandes bases de dados textuais, a tarefa de rotular todos os grupos
encontrados durante o agrupamento, pode ser bastante exaustiva para o usuário.

Dentre os métodos automatizados, é encontrado na literatura dois tipos de abordagens, uma 
baseada em conhecimento interno e a outra baseada em conhecimento externo\cite{Nogueira2013}. 
A primeira se utiliza somente de informações que podem ser obtidas na coleção de documentos, 
como por exemplo a frequência do termo, localização do termo na estrutura do documento.
Enquanto a abordagem de conhecimento externo, levam em considerações também fontes de informação
externas, para auxiliar a escolha dos termos mais representativos. 

Em ambas abordagens a literatura fornece uma ampla gama de métodos, com o objetivo de obter bons
descritores dos grupos. Os descritores podem ser extraídos com os termos mais frequentes no grupo,
, no entanto o resultado pode ser genérico demais\cite{Pucktada2006}, ou os descritores podem
ser extraídos dos grupos que estão mais próximos do centroide do grupo.

Contudo \cite{Nogueira2013} destaca que grande parte dos métodos de extração de descritores 
encontrados na literatura, são embutidos na fase de agrupamento. O que justifica a avaliação 
dos mesmos em função do desempenho do agrupamento. No entanto essa junção da extração de rótulos
na fase de agrupamento, dificulta a combinação de diferentes técnicas de agrupamento e 
consequentemente a escolha de bons descritores. Logo os métodos onde a extração é realizada 
após a fase de agrupamento,
de maneira independente, permitem uma melhor adaptação da proposta de organização flexível de 
documentos para diferentes contextos. Essa flexibilidade possibilitou que a investigação
misturasse diferentes técnicas, permitindo obter melhores resultados.





